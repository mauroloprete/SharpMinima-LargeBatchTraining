---
title: "On large-batch training for deep learning: Generalization GAP and Sharp Minima"
bibliography: references.bib
format:
  revealjs: 
    theme: 
     - dark
nocite: |
  @*
---

## Introducci√≥n

En el proceso de entrenamiento de un modelo de aprendizaje nos enfrentamos al siguiente problema:


$$\min_{x} ~ f(x) := \frac{1}{M} \sum_{i = 1}^{M}{f_{i}(x)}$$

Donde $f_{i}$ refiere a la funci√≥n de perdida de la obseraci√≥n i-√©sima con $i \in {1, 2, \cdots, M}$.

Como toda funci√≥n de perdida, $f_{i}$ captura la desviaci√≥n de la predicci√≥n del modelo sobre los datos de entrenamiento.

## Entrenamiento del modelo


El ajuste o entrenamiento del modelo se reudce a un **problema de optimizaci√≥n** (o se le parece). En particular, se busca encontrar el m√≠nimo de la funci√≥n $f$.

Existen diferentes m√©todos para resolver este problema, la utilizaci√≥n de un algoritmo u otro y su convergencia depende de la naturaleza de la funci√≥n $f$.

- Convexidad
- Continiudad
- Derivabilidad

## Entrenamiento del modelo (cont.)


Un m√©todo bastante utilizado es el m√©todo del gradiente estoc√°stico (SGD) que consiste en un algoritmo recursivo para encontrar un m√≠nimo local de una funci√≥n diferenciable:

$$x_{k+1} = x_{k} - \alpha_{k} \left(\frac{1}{|B_{k}|} \sum_{i \in B_{k}}{\nabla}f_{i}(x_{k})\right)$$

Donde $B_{k}$ es un subconjunto de los datos de entrenamiento y $\alpha_{k}$ es el tama√±o del paso en la iteraci√≥n $k$.

## ¬øQu√© tama√±o de $B_{k}$ es el mejor?

El objetivo general del articulo es medir cambios en la brecha (**GAP**) entre las m√©tricas de training y test al considerar dos estrategias:

- Large Batch (LB)

- Small Batch (SB)

Con una estrateg√≠a de $SB$ estamos considerando $|B_{k}| << M$ donde valores comunes rondan entre $32, 64, \cdots, 512$.

## Contras de LB

1. M√©todos sobreajustan el modelo a los datos de entrenamiento.
2. M√©todos LB son atraidos por puntos de silla.
3. LB pierden capacidad explorativa de los SB, tienden a estancarse en puntos cr√≠ticos locales y no pueden escapar de ellos.
4. LB y SB pueden converger a diferentes puntos cr√≠ticos, el primero con puntos cr√≠ticos de menor calidad.

## Idea principal

- LB al tender a minimizadores bruscos (**sharp**) y esto afecta la generalizaci√≥n del modelo
- LB Min√≠mos mas inestables
- LB Valores propios de la matriz Hessiana de $f$ son mas grandes, lo que afectan la velocidad de convergencia y su alto valor nos indica una curvatura pronunciada en esa direcci√≥n.

¬øPor qu√© LB y no SB?

. . .

*Mas iteraciones para cubir la muestra de entrenamiento, al tener conjunto de datos peque√±os es imposible paralelizarlo.*


## Puntos bruscos (**sharp**) o llanos (**flat**)

De forma gr√°fica, un punto brusco (**sharp**) es aquel que tiene una curvatura pronunciada en una direcci√≥n, mientras que un punto llano (**flat**) es aquel que tiene una curvatura suave en todas las direcciones.

<p align="center">
  <img src="https://raw.githubusercontent.com/mauroloprete/SharpMinima-LargeBatchTraining/main/img/sharp.png" alt="Descripci√≥n de la imagen">
</p>

## Algoritmo ADAM

```plaintext
Inicializar Œ∏‚ÇÄ, m‚ÇÄ, v‚ÇÄ como 0
t ‚Üê 0
Mientras no se haya alcanzado el n√∫mero m√°ximo de iteraciones:
    Obtener el conjunto de datos de entrenamiento X y sus etiquetas Y
    Calcular el gradiente promedio:
    g‚Çú = (1/|Bk|) ‚àë·µ¢ ‚àá_Œ∏ ùìõ(Œ∏_{t-1}, x·µ¢, y·µ¢)
    Incrementar t: t ‚Üê t + 1
    Actualizar los momentos:
    m‚Çú = Œ≤‚ÇÅ ‚ãÖ m_{t-1} + (1 - Œ≤‚ÇÅ) ‚ãÖ g‚Çú
    v‚Çú = Œ≤‚ÇÇ ‚ãÖ v_{t-1} + (1 - Œ≤‚ÇÇ) ‚ãÖ (g‚Çú ‚äô g‚Çú)
    Corregir los momentos sesgados:
    mÃÇ‚Çú = m‚Çú / (1 - Œ≤‚ÇÅ·µó)
    vÃÇ‚Çú = v‚Çú / (1 - Œ≤‚ÇÇ·µó)
    Actualizar los par√°metros:
    Œ∏‚Çú = Œ∏_{t-1} - Œ± ‚ãÖ mÃÇ‚Çú / (‚àö(vÃÇ‚Çú) + Œµ)
```

Donde $\epsilon = 0.001$ y $\beta_{1} = 0.9$ ; $\beta_{2} = 0.99$

## Experimentos

```{r,echo = FALSE}
library(gt)
library(gtExtras)
# Crear los datos
datos <- data.frame(
  Nombre = c("F1", "F2", "C1", "C2", "C3", "C4"),
  `Tipo de red` = c("Completamente conectada", "Completamente conectada", "Shallow Convolutional", "Deep Convolutional", "Shallow Convolutional", "Deep Convolutional"),
  Dataset = c("MNIST", "TIMIT", "CIFAR-10", "CIFAR-10", "CIFAR-100", "CIFAR-100"),
  `Bk LB` = c(6000, 72133, 5000, 5000, 5000, 5000),
  Info = c("Datos de escritura a mano",
           "Conjunto est√°ndar de grabaciones de habla para evaluar sistemas de reconocimiento autom√°tico, con 630 hablantes y transcripciones fon√©ticas.",
           "60,000 im√°genes en color de 10 clases diferentes para tareas de clasificaci√≥n de im√°genes.",
           "60,000 im√°genes en color de 10 clases diferentes para tareas de clasificaci√≥n de im√°genes.",
           "60,000 im√°genes en color de 100 clases diferentes para tareas de clasificaci√≥n de im√°genes.",
           "60,000 im√°genes en color de 100 clases diferentes para tareas de clasificaci√≥n de im√°genes.")
)

# Crear tabla gt
tabla_gt <- datos %>%
  gt() %>%
  tab_header(
    title = "Tabla de Datos",
    subtitle = "Informaci√≥n sobre diferentes conjuntos de datos"
  ) %>%
  fmt_number(
    columns = vars(Bk.LB),
    decimals = 0
  ) %>%
  gt_theme_dark()

tabla_gt

```

La estrategia SB, utilza un tama√±o de lote de 256, para todos los conjuntos de datos.

## Resultados


$$\text{(Precisi√≥n)} = \frac{\text{(TP)}}{\text{(TP)} + \text{(FP)}}
$$

```{r}
library(gt)

data <- data.frame(
  Name = c("F1", "F2", "C1", "C2", "C3", "C4"),
  Training_SB_1 = c("99.66% ¬± 0.05%", "99.99% ¬± 0.03%", "99.89% ¬± 0.02%", "99.99% ¬± 0.04%", "99.56% ¬± 0.44%", "99.10% ¬± 1.23%"),
  Training_LB_2 = c("99.92% ¬± 0.01%", "98.35% ¬± 2.08%", "99.66% ¬± 0.2%", "99.99% ¬± 0.01%", "99.88% ¬± 0.30%", "99.57% ¬± 1.84%"),
  Test_SB_3 = c("98.03% ¬± 0.07%", "64.02% ¬± 0.2%", "80.04% ¬± 0.12%", "89.24% ¬± 0.12%", "49.58% ¬± 0.39%", "63.08% ¬± 0.5%"),
  Test_LB_4 = c("97.81% ¬± 0.07%", "59.45% ¬± 1.05%", "77.26% ¬± 0.42%", "87.26% ¬± 0.07%", "46.45% ¬± 0.43%", "57.81% ¬± 0.17%")
)

# Renombrar columnas
colnames(data) <- c("Name", "Training_SB_1", "Training_LB_2", "Test_SB_3", "Test_LB_4")

# Crear la tabla gt
tabla_gt <- data %>%
  gt() %>%
  tab_header(
    title = "Precisi√≥n",
    subtitle = "Precisi√≥n de Entrenamiento (SB) y Test (LB)"
  ) %>%
  cols_label(
    Training_SB_1 = "Training (SB)",
    Training_LB_2 = "Training (LB)",
    Test_SB_3 = "Test (SB)",
    Test_LB_4 = "Test (LB)"
  ) %>%
  fmt_number(
    columns = vars(Training_SB_1, Training_LB_2, Test_SB_3, Test_LB_4),
    pattern = "{x}"
  ) %>%
  gt_theme_dark()

# Mostrar la tabla gt

  

# Mostrar la tabla gt
tabla_gt
```

## Representaci√≥n param√©trica en una dimensi√≥n

Si tomamos $x_{s}^{*}$ y $x_{l}^{*}$ como los m√≠nimos de $f$ para $SB$ y $LB$ respectivamente podemos gr√°ficar en un segmento que contenga a ambos puntos:

$$f(\alpha x_{s}^{*} + (1 - \alpha)x_{l}^{*})$$

Con $\alpha \in [-1, 2]$.

En $\alpha = 0$ tenemos el m√≠nimo de $LB$ y en $\alpha = 1$ el m√≠nimo de $SB$.


## Representaci√≥n param√©trica en una dimensi√≥n (cont.)

![](https://raw.githubusercontent.com/mauroloprete/SharpMinima-LargeBatchTraining/main/img/f1f2.png)

## Representaci√≥n param√©trica en una dimensi√≥n (cont.)

![](https://raw.githubusercontent.com/mauroloprete/SharpMinima-LargeBatchTraining/main/img/c1c2.png)


## Representaci√≥n param√©trica en una dimensi√≥n (cont.)

![](https://raw.githubusercontent.com/mauroloprete/SharpMinima-LargeBatchTraining/main/img/c3c4.png)

## Representaci√≥n param√©trica en una dimensi√≥n (cont.)


```{python,eval = FALSE, echo = TRUE}
alpha_range = numpy.linspace(-1, 2, 25)
data_for_plotting = numpy.zeros((25, 4))

i = 0
for alpha in alpha_range:
    for p in range(len(sb_solution)):
        model.trainable_weights[p].set_value(lb_solution[p]*alpha +
                                             sb_solution[p]*(1-alpha))
    train_xent, train_acc = model.evaluate(X_train, Y_train,
                                           batch_size=5000, verbose=0)
    test_xent, test_acc = model.evaluate(X_test, Y_test,
                                         batch_size=5000, verbose=0)
    data_for_plotting[i, :] = [train_xent, train_acc, test_xent, test_acc]
    i += 1
```

[C√≥digo obtenido de Github de los mismos autores](https://github.com/keskarnitish/large-batch-training/blob/master/plot_parametric_plot.py)

## Sharpness de los m√≠nimos

La forma de los m√≠nimos puede caracterizarse por los valores propios de la matriz Hessiana de $f$ en el m√≠nimo, en un contexto de optimizaci√≥n cl√°sico mientras que en el ambito del aprendizaje tiene un alto costo computacional.

. . .

Al considerar un m√≠nimo en una sola parte de $R^{n}$, ver un conjunto del espacio total de tama√±o $p$ (variedades) para hacer esto se utiliza una matriz de $n$ filas y $p$ columnas generadas de forma aleatoria.

## Sharpness de los m√≠nimos (cont.)

Para asegurar la invarianza por la dimensionalidad y la dispersi√≥n se define un conjunto $C_{\epsilon}$ como:

$$C_\varepsilon = \{ z \in \mathbb{R}^p : -\varepsilon (|(A+x)_i|+1) \leq z_i \leq \varepsilon (|(A+x)_i|+1)\}$$
$$ \quad \forall i \in \{1,2,\dots,p\}$$

Donde $A^{+}$ es la matriz pseudo-inversa de $A$ y $\varepsilon$ es un par√°metro que controla el tama√±o de la caja.

## Sharpness de los m√≠nimos (cont.)

$$\varphi_{x,f}(\varepsilon,A) := \frac{\left( \max_{y \in C_\varepsilon} f(x+Ay) \right) - f(x)}{1+f(x)} \times 100$$


Esta m√©trica se relaciona con el valor propio mas gr√°nde  de la matriz Hessiana de $f$ en el m√≠nimo y en el caso de considerar $A$ se aproxima a los valores de $Ritz$

## Sharpness de los m√≠nimos (cont.)

<p align="center">
  <img src="https://raw.githubusercontent.com/mauroloprete/SharpMinima-LargeBatchTraining/main/img/sharpness.png" alt="Descripci√≥n de la imagen" class="imagen-escalada">
</p>

## SB >> LB

- **Comportamiento de SB (Small Batch) con gradientes ruidosos:**
  - Los gradientes ruidosos en m√©todos SB afectan el movimiento de las iteraciones.
  . . .
  - El ruido en el gradiente aleja las iteraciones de los minimizadores afilados.
  . . .
  - Promueve el movimiento hacia minimizadores m√°s planos para evitar la influencia del ruido.
  

## SB >> LB (cont.)

- **Impacto del tama√±o del lote:**
  - Cuando el tama√±o del lote excede un umbral espec√≠fico:
    - El ruido en el gradiente estoc√°stico no es suficiente para salir de la cuenca inicial.
    . . .
    - Conduce a la convergencia hacia un tipo de minimizador m√°s afilado.
  

## LB escapando del m√≠nimo puntiagudo

Para mostrar como LB escapa de los m√≠nimos se entreno una red en 100 iteraciones con un tama√±o de lote de 256 (SB) y el resultado se utilizo como punto de partida para una estrategia LB.

<p align="center">
  <img src="https://raw.githubusercontent.com/mauroloprete/SharpMinima-LargeBatchTraining/main/img/piggy.png" alt="Descripci√≥n de la imagen">
</p>

## Preguntas abiertas

- ¬øLos m√©todos de gran lote siempre convergen hacia minimizadores afilados en el aprendizaje profundo?
. . .
- ¬øSe pueden dise√±ar redes que funcionen mejor con m√©todos de gran lote?
- ¬øHay una manera de comenzar las redes para que los m√©todos de gran lote funcionen mejor?
. . .
- ¬øPodemos encontrar formas de evitar que los m√©todos de gran lote se enfoquen en minimizadores afilados?

## Conclusiones

- La convergencia hacia minimizadores afilados afecta la capacidad de generalizaci√≥n en m√©todos de gran lote para el aprendizaje profundo.
. . .
- Estrategias previas como la ampliaci√≥n de datos y el entrenamiento conservador no resuelven completamente el problema de generalizaci√≥n deficiente en m√©todos de gran lote.
. . .
- El muestreo din√°mico muestra cierta promesa para mejorar los resultados de estos m√©todos.
. . .
- La similitud en los valores de p√©rdida entre minimizadores afilados y planos es consistente con estudios previos.

## Referencias

::: {#refs}
:::